{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 â€” dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Hadoop Distributed File System (HDFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark helps us for computation of big data sets. Hadoop has also this functionality with mapreduce but it is way faster spark. However, Spark does not have a file management system, whereas hadoop does. Here you can find a tutorial of how this HDFS works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hdfs [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
      "\n",
      "  OPTIONS is none or any of:\n",
      "\n",
      "--buildpaths                       attempt to add class files from build tree\n",
      "--config dir                       Hadoop config directory\n",
      "--daemon (start|status|stop)       operate on a daemon\n",
      "--debug                            turn on shell script debug mode\n",
      "--help                             usage information\n",
      "--hostnames list[,of,host,names]   hosts to use in worker mode\n",
      "--hosts filename                   list of hosts to use in worker mode\n",
      "--loglevel level                   set the log4j level for this command\n",
      "--workers                          turn on worker mode\n",
      "\n",
      "  SUBCOMMAND is one of:\n",
      "\n",
      "\n",
      "    Admin Commands:\n",
      "\n",
      "cacheadmin           configure the HDFS cache\n",
      "crypto               configure HDFS encryption zones\n",
      "debug                run a Debug Admin to execute HDFS debug commands\n",
      "dfsadmin             run a DFS admin client\n",
      "dfsrouteradmin       manage Router-based federation\n",
      "ec                   run a HDFS ErasureCoding CLI\n",
      "fsck                 run a DFS filesystem checking utility\n",
      "haadmin              run a DFS HA admin client\n",
      "jmxget               get JMX exported values from NameNode or DataNode.\n",
      "oev                  apply the offline edits viewer to an edits file\n",
      "oiv                  apply the offline fsimage viewer to an fsimage\n",
      "oiv_legacy           apply the offline fsimage viewer to a legacy fsimage\n",
      "storagepolicies      list/get/set block storage policies\n",
      "\n",
      "    Client Commands:\n",
      "\n",
      "classpath            prints the class path needed to get the hadoop jar and\n",
      "                     the required libraries\n",
      "dfs                  run a filesystem command on the file system\n",
      "envvars              display computed Hadoop environment variables\n",
      "fetchdt              fetch a delegation token from the NameNode\n",
      "getconf              get config values from configuration\n",
      "groups               get the groups which users belong to\n",
      "lsSnapshottableDir   list all snapshottable dirs owned by the current user\n",
      "snapshotDiff         diff two snapshots of a directory or diff the current\n",
      "                     directory contents with a snapshot\n",
      "version              print the version\n",
      "\n",
      "    Daemon Commands:\n",
      "\n",
      "balancer             run a cluster balancing utility\n",
      "datanode             run a DFS datanode\n",
      "dfsrouter            run the DFS router\n",
      "diskbalancer         Distributes data evenly among disks on a given node\n",
      "httpfs               run HttpFS server, the HDFS HTTP Gateway\n",
      "journalnode          run the DFS journalnode\n",
      "mover                run a utility to move block replicas across storage types\n",
      "namenode             run the DFS namenode\n",
      "nfs3                 run an NFS version 3 gateway\n",
      "portmap              run a portmap service\n",
      "secondarynamenode    run the DFS secondary namenode\n",
      "zkfc                 run the ZK Failover Controller daemon\n",
      "\n",
      "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
     ]
    }
   ],
   "source": [
    "!hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 items\n",
      "drwxrwxrwt   - yarn   hadoop          0 2020-04-07 02:03 /app-logs\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2020-03-02 09:56 /apps\n",
      "drwxr-xr-x   - yarn   hadoop          0 2020-02-04 08:46 /ats\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2020-02-04 08:47 /atsv2\n",
      "drwxr-xr-x   - basil  hdfs            0 2020-04-08 12:26 /cs449\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2020-02-04 08:47 /hdp\n",
      "drwxr-xr-x   - suresh hdfs            0 2020-02-11 16:43 /ix\n",
      "drwxr-xr-x   - mapred hdfs            0 2020-02-04 08:47 /mapred\n",
      "drwxrwxrwx   - mapred hadoop          0 2020-02-04 08:47 /mr-history\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2020-02-04 08:46 /services\n",
      "drwxrwxrwx   - spark  hadoop          0 2020-04-10 18:06 /spark2-history\n",
      "drwxrwxrwx   - hdfs   hdfs            0 2020-02-23 23:49 /tmp\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2020-04-07 02:03 /user\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2020-02-04 08:47 /warehouse\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls / # these are all the hadoop folders, we can see here the /ix folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 suresh hdfs  745096004 2020-02-11 13:42 /ix/ml-20m/genome-scores.txt\r\n",
      "-rw-r--r--   3 suresh hdfs      40652 2020-02-11 13:42 /ix/ml-20m/genome-tags.txt\r\n",
      "-rw-r--r--   3 suresh hdfs    2538070 2020-02-11 13:42 /ix/ml-20m/movies.txt\r\n",
      "-rw-r--r--   3 suresh hdfs 1493457002 2020-02-11 13:42 /ix/ml-20m/ratings.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /ix/ml-20m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/salvia/LAB3\r\n"
     ]
    }
   ],
   "source": [
    "!pwd # this is the linux space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 items\r\n",
      "drwxrwxrwt   - yarn   hadoop          0 2020-04-07 02:03 /app-logs\r\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2020-03-02 09:56 /apps\r\n",
      "drwxr-xr-x   - yarn   hadoop          0 2020-02-04 08:46 /ats\r\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2020-02-04 08:47 /atsv2\r\n",
      "drwxr-xr-x   - basil  hdfs            0 2020-04-08 12:26 /cs449\r\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2020-02-04 08:47 /hdp\r\n",
      "drwxr-xr-x   - suresh hdfs            0 2020-02-11 16:43 /ix\r\n",
      "drwxr-xr-x   - mapred hdfs            0 2020-02-04 08:47 /mapred\r\n",
      "drwxrwxrwx   - mapred hadoop          0 2020-02-04 08:47 /mr-history\r\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2020-02-04 08:46 /services\r\n",
      "drwxrwxrwx   - spark  hadoop          0 2020-04-10 18:10 /spark2-history\r\n",
      "drwxrwxrwx   - hdfs   hdfs            0 2020-02-23 23:49 /tmp\r\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2020-04-07 02:03 /user\r\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2020-02-04 08:47 /warehouse\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls / # this is the hadoop file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ix-lab3-handout.pdf  lab3-recsys.ipynb\tREADME.md\t\ttest.txt\r\n",
      "lab3-cluster.ipynb   most-rated.pickle\tselected-movies.pickle\ttext.txt\r\n",
      "lab3-dimred.ipynb    rate-movies.py\tsnippets.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: Permission denied: user=salvia, access=WRITE, inode=\"/\":hdfs:hdfs:drwxr-xr-x\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /prova # We cannot manage this files... :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"tagId\": 1124, \"tag\": \"writing\"}\r\n",
      "{\"tagId\": 1125, \"tag\": \"wuxia\"}\r\n",
      "{\"tagId\": 1126, \"tag\": \"wwii\"}\r\n",
      "{\"tagId\": 1127, \"tag\": \"zombie\"}\r\n",
      "{\"tagId\": 1128, \"tag\": \"zombies\"}\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /ix/ml-20m/genome-tags.txt | tail -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. New section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(\"/ix/ml-20m/genome-tags.txt\").map(json.loads)\n",
    "tag2name = dict(data.map(itemgetter(\"tagId\", \"tag\")).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[3] at RDD at PythonRDD.scala:49\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
